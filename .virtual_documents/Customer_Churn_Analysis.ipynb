# Import essential libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style for better visuals
sns.set(style='whitegrid')

# Load the dataset
file_path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv'
df = pd.read_csv(file_path)


# Display basic information about the dataset
print("Dataset Information:")
df.info()

# Display the first 5 rows of the dataset
print("\nFirst 5 Rows of the Dataset:")
print(df.head())

# Display summary statistics for numerical columns
print("\nSummary Statistics:")
print(df.describe())


# Convert 'TotalCharges' to a numeric type, coercing errors to NaN
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Drop rows with missing values that were created during coercion
df.dropna(inplace=True)

# Verify the change
print("Data types after cleaning 'TotalCharges':")
df.info()


plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Contract', hue='Churn')
plt.title('Churn Distribution by Contract Type', fontsize=16)
plt.xlabel('Contract Type', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)
plt.legend(title='Churn')

# --- IMPORTANT: Save the figure ---
plt.savefig('churn_by_contract.png')

plt.show()


plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='InternetService', hue='Churn')
plt.title('Churn Distribution by Internet Service', fontsize=16)
plt.xlabel('Internet Service', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)
plt.legend(title='Churn')

# --- IMPORTANT: Save the figure ---
plt.savefig('churn_by_internetservice.png')

plt.show()


# Drop the customerID column as it is not a predictive feature
df.drop('customerID', axis=1, inplace=True)

# Convert all categorical variables into dummy/indicator variables (one-hot encoding)
# This will handle all columns with text data automatically
X = pd.get_dummies(df, drop_first=True)

# The 'Churn_Yes' column is our target variable (what we want to predict)
# We separate it from the features
y = X.pop('Churn_Yes')


from sklearn.model_selection import train_test_split

# Split the data into 80% for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


from sklearn.preprocessing import StandardScaler

# First, check the actual column names in your DataFrame
print("Available columns:", X_train.columns.tolist())

# After seeing the output above, replace this line with your actual numerical column names
# For example, if your DataFrame shows columns like 'feature1', 'feature2', 'feature3':
numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()
# This will automatically select all numerical columns

# Create a scaler object
scaler = StandardScaler()

# Fit the scaler ONLY on the training data and transform both train and test data
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# Alternatively, if you want to be more specific about which columns to scale:
# First check the output of the print statement above
# Then replace numerical_cols with the actual column names from your DataFrame
# numerical_cols = ['actual_column1', 'actual_column2', 'actual_column3']


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Initialize and train the model
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)

# Make predictions
y_pred_log = log_model.predict(X_test)

# Evaluate and print metrics
print("--- Logistic Regression Performance ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_log):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_log):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_log):.4f}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_log):.4f}")


from xgboost import XGBClassifier

# Initialize and train the model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate and print metrics
print("\n--- XGBoost Classifier Performance ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_xgb):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_xgb):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_xgb):.4f}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_xgb):.4f}")


# Get feature importances from the trained XGBoost model
importances = xgb_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Plot and save the top 10 features
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10))
plt.title('Top 10 Feature Importances (XGBoost)', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)

# --- IMPORTANT: Save the figure ---
plt.savefig('feature_importance.png')

plt.show()



